{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "language_modeling.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TasnubaS/Random-Solutions/blob/master/language_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_YdG5iIyUlt"
      },
      "source": [
        "# Statistical Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tc_UrWwyUlv"
      },
      "source": [
        "import urllib.request\n",
        "from os.path import isfile\n",
        "if not isfile(\"lang-model.txt\"):\n",
        "    url = \"https://yangfengji.net/uva-nlp-course/data/lang-model.txt.zip\"\n",
        "    print(\"Downloading ...\")\n",
        "    filename, headers = urllib.request.urlretrieve(url, filename=\"lang-model.txt.zip\")\n",
        "\n",
        "    print(\"Decompressing the file ...\")\n",
        "    !unzip lang-model.txt.zip\n",
        "\n",
        "sents = open(\"lang-model.txt\").read().split(\"\\n\")\n",
        "print(\"Read {} sentences\".format(len(sents)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "228thlmNyUlw"
      },
      "source": [
        "from collections import defaultdict\n",
        "from math import log2, pow\n",
        "from numpy.random import choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1iDs5NEyUlw"
      },
      "source": [
        "class BigramLM(object):\n",
        "    def __init__(self):\n",
        "        self.vocab = {\"<start>\":0, \"<end>\":1}\n",
        "        self.model = {}\n",
        "        self.tok_counter = '__total__'\n",
        "\n",
        "    def build(self, fname):\n",
        "        \"\"\" Build a Bigram LM\n",
        "        \"\"\"\n",
        "        fin = open(fname)\n",
        "        for line in fin:\n",
        "            tokens = line.strip().split()\n",
        "            L = len(tokens)\n",
        "            for i in range(1, L):\n",
        "                prev_tok = tokens[i-1]\n",
        "                curr_tok = tokens[i]\n",
        "                if curr_tok not in self.vocab:\n",
        "                    self.vocab.update({curr_tok : len(self.vocab)})\n",
        "                try:\n",
        "                    self.model[prev_tok][curr_tok] += 1.0\n",
        "                except KeyError:\n",
        "                    self.model[prev_tok] = defaultdict(float)\n",
        "                    self.model[prev_tok][curr_tok] += 1.0\n",
        "                self.model[prev_tok][self.tok_counter] += 1.0\n",
        "        # Normalization\n",
        "        for (prev_tok, dct) in self.model.items():\n",
        "            for (curr_tok, val) in self.model[prev_tok].items():\n",
        "                if curr_tok != self.tok_counter: # to avoid normalizing the counter token\n",
        "                    self.model[prev_tok][curr_tok] /= self.model[prev_tok][self.tok_counter]\n",
        "        print(\"Done with modeling building\\nVocab size = {}\".format(len(self.vocab)))\n",
        "            \n",
        "\n",
        "    def eval(self, text):\n",
        "        \"\"\" Evaluate a given text\n",
        "        \"\"\"\n",
        "        tokens = text.strip().split()\n",
        "        L = len(tokens)\n",
        "        logprob = 0.0\n",
        "        for i in range(1, L):\n",
        "            prev_tok = tokens[i-1]\n",
        "            curr_tok = tokens[i]\n",
        "            if prev_tok not in self.vocab:\n",
        "                prev_tok = 'UNK'\n",
        "            if curr_tok not in self.vocab:\n",
        "                curr_tok = 'UNK'\n",
        "            try:\n",
        "                logprob += log2(self.model[prev_tok][curr_tok])\n",
        "            except ValueError:\n",
        "                print(\"{} -> {}\".format(prev_tok, curr_tok))\n",
        "                logprob += -100 # A large number, technically this should be infty\n",
        "        # Compute PPLx\n",
        "        pplx = pow(2, -1*logprob/(L-1))\n",
        "        return pplx\n",
        "\n",
        "\n",
        "    def generate(self, method=\"random\", length=20):\n",
        "        \"\"\" Random sampling words from this model for generation\n",
        "        \"\"\"\n",
        "        text = []\n",
        "        prev_tok = \"<start>\"\n",
        "        text.append(prev_tok)\n",
        "        while (prev_tok != \"<end>\") and (len(text) <= length):\n",
        "            tokens, probs = [], []\n",
        "            # The following for loop is time-consuming\n",
        "            # For large-scale text generation, a pre-processing may be necessary \n",
        "            for (tok, prob) in self.model[prev_tok].items():\n",
        "                if tok != self.tok_counter:\n",
        "                    tokens.append(tok)\n",
        "                    probs.append(prob)\n",
        "            widx = choice(len(probs), 1, p=probs)[0]\n",
        "            prev_tok = tokens[widx]\n",
        "            text.append(prev_tok)\n",
        "        return text            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzRxt2JhyUlx"
      },
      "source": [
        "bigram = BigramLM()\n",
        "bigram.build(\"lang-model.txt\")\n",
        "text = \"<start> MY notes on deep learning for nlp <end>\"\n",
        "pplx = bigram.eval(text)\n",
        "print(\"Text = {}\\nPPLx = {:.4f}\".format(text, pplx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChVBchzByUlx"
      },
      "source": [
        "text = bigram.generate()\n",
        "print(\"Generated text = {}\".format(\" \".join(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-R11-cyUlx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}