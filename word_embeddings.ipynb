{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "word_embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TasnubaS/Random-Solutions/blob/master/word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQclGXV9PzQG"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mewDhlvoPzQJ"
      },
      "source": [
        "Outline of this notebook\n",
        "\n",
        "- Latent semantic analysis (SVD)\n",
        "- Skip-gram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6YYLRzhPzQK"
      },
      "source": [
        "## 0. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hyRba7OzPzQK"
      },
      "source": [
        "import urllib.request\n",
        "from os.path import isfile\n",
        "if not isfile(\"abstract-filtered.txt\"):\n",
        "    url = \"https://yangfengji.net/uva-nlp-course/data/abstract-filtered.txt.zip\"\n",
        "    print(\"Downloading ...\")\n",
        "    filename, headers = urllib.request.urlretrieve(url, filename=\"abstract-filtered.txt.zip\")\n",
        "\n",
        "    print(\"Decompressing the file ...\")\n",
        "    !unzip abstract-filtered.txt.zip\n",
        "\n",
        "sents = open(\"abstract-filtered.txt\").read().split(\"\\n\")\n",
        "print(\"Read {} sentences\".format(len(sents)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbwjzIQwPzQL"
      },
      "source": [
        "## 1. Latent Semantic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgfYUboVPzQM"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.decomposition import TruncatedSVD as SVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pza-w7IFPzQM"
      },
      "source": [
        "### 1.1 Construct sent-word matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL9FxA94PzQN"
      },
      "source": [
        "The following code will \n",
        "\n",
        "- construct a data matrix with size: words x sentences\n",
        "- build the vocab (named vocab1), which maps a word to its index\n",
        "- build the vocab (named ivocab1), which maps an index to its corresponding word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnOA-zixPzQN"
      },
      "source": [
        "vectorizer = CountVectorizer(lowercase=True, min_df=5, max_df=1.0, ngram_range=(1,1))\n",
        "mat = vectorizer.fit_transform(sents) # Dim: Sent x Word\n",
        "vocab1 = vectorizer.vocabulary_\n",
        "ivocab1 = {val:key for (key, val) in vocab1.items()}\n",
        "mat = mat.asfptype().T\n",
        "print(\"Matrix shape = {}\".format(mat.shape)) # Words x Texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ZlGtosPzQO"
      },
      "source": [
        "### 1.2 SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkhkm7-jPzQO"
      },
      "source": [
        "For a given matrix $\\bf{M}\\in\\mathbb{R}^{v\\times m}$, SVD decompose the matrix into three components with a predefined parameter $k$\n",
        "$$\\bf{M} = \\bf{U}\\cdot\\bf{D}\\cdot\\bf{V}^{t}$$\n",
        "where\n",
        "\n",
        "- $\\bf{U}\\in\\mathbb{R}^{v\\times k}$\n",
        "- $\\bf{D}\\in\\mathbb{R}^{k}$: the elements of the diagnoal matrix\n",
        "- $\\bf{V}^{t}\\in\\mathbb{R}^{k\\times m}$\n",
        "\n",
        "The word embeddings we get from SVD is \n",
        "\n",
        "$$\\bf{W} = \\bf{U}\\cdot\\bf{D}$$\n",
        "\n",
        "where each column is a word embedding for the corresponding word in the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8KU_aXOPzQO"
      },
      "source": [
        "k_max = 500\n",
        "svd = SVD(n_components=k_max)\n",
        "W1 = svd.fit_transform(mat) # = U*D, Size: Word x k_max\n",
        "print(W1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtXaWtksPzQP"
      },
      "source": [
        "The plot of the singular values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4d5jociPzQP"
      },
      "source": [
        "sigma = svd.singular_values_\n",
        "plt.plot(range(len(sigma)), sigma, '.')\n",
        "plt.ylim((0, 280))\n",
        "plt.xlabel(\"Dimension indices\")\n",
        "plt.ylabel(\"Singular values\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35fCuWqUPzQP"
      },
      "source": [
        "### 1.3 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzJzX266PzQQ"
      },
      "source": [
        "def print_sim_words(cossim, vocab, ivocab, word='embeddings'):\n",
        "    widx = vocab[word] # get word index\n",
        "    sim_scores = cossim[:,widx] # get similarity scores\n",
        "    # print(sim_scores)\n",
        "    sim_indices = np.argsort(sim_scores)[::-1] # rank the similarity score with descreasing order\n",
        "    sim_words = [ivocab[widx] for widx in sim_indices] # rank the words based on their similarity scores\n",
        "    print(sim_words[:20]) # print out the first 20 words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBJckFsjPzQQ"
      },
      "source": [
        "Based on the new representations of words $\\bf{W}$, for a given word $x$, we can use cosine similarity to find the similar words in the vocab,\n",
        "\n",
        "$$\\cos(x,x') = \\frac{\\langle\\bf{w}_{x},\\bf{w}_{x'}\\rangle}{\\|\\bf{w}_{x}\\|_2\\|\\bf{w}_{x'}\\|_2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-gVhrcPzQR"
      },
      "source": [
        "# Compute the cosine similarity based on word embeddings\n",
        "\n",
        "cossim1 = cosine_similarity(W1,W1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWd-qBH4PzQR"
      },
      "source": [
        "# Print the top 20 similar words\n",
        "\n",
        "print_sim_words(cossim1, vocab1, ivocab1, word='embeddings')\n",
        "# print(ivocab1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "586zhw6kPzQS"
      },
      "source": [
        "## 2. Skip-gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4fQWz8kPzQT"
      },
      "source": [
        "### 2.1 The implementation from fastText\n",
        "\n",
        "In this section, we will first use the implementation from the [fastText](https://pypi.org/project/fasttext/) to do some preliminary study of the skip-gram model. \n",
        "This code fully implements the technical details as we discussed in class. \n",
        "Please refer to the documentation of fastText for more information of using this code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVw5nDUrPzQT"
      },
      "source": [
        "import fasttext\n",
        "\n",
        "model = fasttext.train_unsupervised('data/arxiv/abstract-filtered.txt', model='skipgram',\n",
        "                                    ws = 3, # context window size \n",
        "                                    dim = 50, # word embedding dimension\n",
        "                                    epoch = 3, # training epochs\n",
        "                                    minCount=5) # the minimal count of words in the vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rsdMOPcPzQU"
      },
      "source": [
        "After training the model, we can collect the word embedding metrices and vocabulary for evaluation purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xHSOOqCPzQU"
      },
      "source": [
        "W2 = model.get_output_matrix()\n",
        "vocab2 = {word:idx for (idx, word) in enumerate(model.get_words())}\n",
        "ivocab2 = {idx:word for (idx, word) in enumerate(model.get_words())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9n1nkNLPzQV"
      },
      "source": [
        "Compute the cosine similarity of all the words in the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmdQPBwYPzQW"
      },
      "source": [
        "cossim2 = cosine_similarity(W2,W2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDTYQ2QFPzQX"
      },
      "source": [
        "Now, we can pick any word from the vocab and find out its similar words based on the cosine similarity of word embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVAwEzOAPzQZ"
      },
      "source": [
        "# Print the top 20 similar words\n",
        "\n",
        "print_sim_words(cossim2, vocab2, ivocab2, word='embeddings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIOeh4WJPzQa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}