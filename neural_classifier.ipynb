{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "neural_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TasnubaS/Random-Solutions/blob/master/neural_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILGKmyykPz0M"
      },
      "source": [
        "# Downgrade torchtext to 0.8.1\n",
        "% pip install torchtext==0.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSPIBRNuxYin"
      },
      "source": [
        "# Neural Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzaUL0_xYiq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fa00642-1cfe-4c9e-9d86-729e42761e01"
      },
      "source": [
        "# Necessary packages\n",
        "\n",
        "import sys, os, random, math, sys\n",
        "import torch, spacy\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "## Random seeds, to make the results reproducible\n",
        "seed = 1\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(torch.randn(5))\n",
        "\n",
        "\n",
        "## Download the data\n",
        "!rm -rf sst sst.zip\n",
        "import urllib.request\n",
        "url = \"https://yangfengji.net/uva-nlp-course/data/sst.zip\"\n",
        "filename, headers = urllib.request.urlretrieve(url, filename=\"sst.zip\")\n",
        "!unzip sst.zip\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n",
            "Archive:  sst.zip\n",
            "   creating: sst/\n",
            "  inflating: sst/tst.tsv             \n",
            "  inflating: sst/dictionary.txt      \n",
            "  inflating: sst/val.tsv             \n",
            "  inflating: sst/trn.tsv             \n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFsq6H-wxYir"
      },
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4HwkptxYir"
      },
      "source": [
        "The following **reader()** function will do three things\n",
        "\n",
        "- read texts from files (line 22 - 25)\n",
        "- build a vocab based on training data (line 29)\n",
        "- convert each set (training, val, test) into a collection of mini-batch vector (line 35 - 39)\n",
        "\n",
        "The **input** to the neural network model defined in the next section is mini-batch word index matrix $\\bf{X}$\n",
        "$$\\bf{X} = [\\bf{x},\\dots,\\bf{x}_B]$$\n",
        "where each $\\bf{x}_i$ is a **column** vector containing word indices from an input text. $B$ is the mini-batch size as explained in the lacture.\n",
        "\n",
        "Sentences in a mini-batch usually have different lengths. To form a matrix $\\bf{X}$, a simple trick is to add a special token \\<pad\\> at the end of shorter sentences. For example, consider the two text examples in the lecture \n",
        "\n",
        "- I like coffee .\n",
        "- I don t like tea .\n",
        "\n",
        "To make these two examples have the same length, the preprocessing code will add two special \\<pad\\> tokens at the end of the first sentence \n",
        "\n",
        "- I like coffee . \\<pad\\> \\<pad\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSaVFGzWxYir"
      },
      "source": [
        "### 1.1 Define the data processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-T0beERxYis"
      },
      "source": [
        "from torchtext.data import Field, ReversibleField, Dataset, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Download the 'en' module for spaCy using\n",
        "#  'python -m spacy download en'\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_fn(text):\n",
        "    \"\"\" Tokenization function\n",
        "    \"\"\"\n",
        "    # return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "    return text.strip().split()\n",
        "\n",
        "\n",
        "def reader(suffix=\".tsv\", rpath=\"sst\", batch_size=8, min_freq=2):\n",
        "    \"\"\"\n",
        "    - suffix: data file suffix\n",
        "    - rpath: path to the data files\n",
        "    - batch_size: mini-batch size\n",
        "    - min_freq: word frequency cutoff, frequency less than min_freq will be removed when building the vocab\n",
        "    \"\"\"\n",
        "    # Utterance Field: text\n",
        "    TXT = Field(sequential=True, tokenize=tokenize_fn, init_token=None, eos_token=None, lower=True)\n",
        "    LABEL = Field(sequential=False, unk_token=None, dtype=torch.long, use_vocab=False)\n",
        "    \n",
        "    # Create a Dataset instance\n",
        "    fields = [(\"label\", LABEL), (\"text\", TXT)]\n",
        "    trn_data = TabularDataset(os.path.join(rpath,'trn'+suffix), format=\"TSV\", fields=fields, skip_header=False)\n",
        "    val_data = TabularDataset(os.path.join(rpath,'val'+suffix), format=\"TSV\", fields=fields, skip_header=False)\n",
        "    tst_data = TabularDataset(os.path.join(rpath,'tst'+suffix), format=\"TSV\", fields=fields, skip_header=False)\n",
        "    \n",
        "    # Split\n",
        "    # Build vocab using training data\n",
        "    TXT.build_vocab(trn_data, min_freq=min_freq) # or max_size=10000\n",
        "    # \n",
        "    train_iter, val_iter, test_iter = BucketIterator.splits((trn_data, val_data, tst_data), # data\n",
        "                                                             batch_size=batch_size, # \n",
        "                                                             sort=True, # sort_key not specified\n",
        "                                                             sort_key = lambda x : len(x.text),\n",
        "                                                             shuffle=False, # shuffle between epochs\n",
        "                                                             repeat=False)\n",
        "    return train_iter, val_iter, test_iter, TXT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DDQGipUxYis"
      },
      "source": [
        "### 1.2 Processing the SST dataset\n",
        "\n",
        "Calling the previously defined functions to process the data. Potentially useful hyper-parameters in the following code block\n",
        "\n",
        "- **batch_size**: the number of training examples in each mini batch, default value: 16\n",
        "- **min_freq**: the cutoff threshold of low-frequency words, default value: 1 (no cutoff)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4i_betyxYit"
      },
      "source": [
        "train_iter, val_iter, test_iter, txtfield = reader(suffix=\".tsv\", rpath=\"sst\", batch_size=16, min_freq=1)\n",
        "vocab_size = len(txtfield.vocab)\n",
        "# print(txtfield.vocab.freqs)\n",
        "print(\"Vocab size = {}\".format(vocab_size))\n",
        "pad = txtfield.vocab.stoi[txtfield.pad_token]\n",
        "\n",
        "print(\"[TRAIN]:%d (dataset:%d)\\t[VAL]:%d (dataset:%d)\\t[TEST]:%d (dataset:%d)\"\n",
        "    % (len(train_iter), len(train_iter.dataset),\n",
        "    len(val_iter), len(val_iter.dataset),\n",
        "    len(test_iter), len(test_iter.dataset)))\n",
        "print(\"[vocab]:%d\" % (vocab_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9GY_kwixYit"
      },
      "source": [
        "## 2. A Simple Feed-forward Neural Network Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6587PyWgxYiu"
      },
      "source": [
        "The goal of this section is to demonstrate \n",
        "\n",
        "- how to define a simple neural text classifier\n",
        "- how to train the neural network model with the BP algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY6BWIBbxYiu"
      },
      "source": [
        "### 2.1 Define Neural Network Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kqrjcd7xYiu"
      },
      "source": [
        "This section defines a basic neural network model (as illustrated in the following figure) for text classification. \n",
        "\n",
        "<img src=\"https://yangfengji.net/uva-nlp-course/code/figures/nns.png\" alt=\"drawing\" width=\"300\"/>\n",
        "\n",
        "There are two basic components of definiing neural network models in PyTorch\n",
        "\n",
        "- __ init __ (): Initialize an instance\n",
        "- forward(): Forward computation, the network architecture is specified in this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC8DEpL6xYiv"
      },
      "source": [
        "class NeuralClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, drop_rate=0.0, class_size=2, pad=1):\n",
        "        super(NeuralClassifier, self).__init__()\n",
        "        \"\"\" Initialization\n",
        "        - vocab_size\n",
        "        - embed_size: word embedding size\n",
        "        - drop_rate: dropout rate\n",
        "        - class_size: number of classes. For binary classification, class_size = 2\n",
        "        \"\"\"\n",
        "        # ---------------------------------\n",
        "        # Configuration\n",
        "        self.vocab_size = vocab_size # size of the vocab\n",
        "        self.class_size = class_size # number of classes\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.pad = pad\n",
        "        # ---------------------------------\n",
        "        # Network parameters\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad)\n",
        "        self.fc = nn.Linear(embed_size, class_size, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        \"\"\" Forward function\n",
        "        \"\"\"\n",
        "        input, label = batch.text, batch.label\n",
        "        \n",
        "        # ---------------------------------\n",
        "        # === Hidden layer ===\n",
        "        # Sum over all the embeddings for each input text\n",
        "        #   then, pass through the nonlinear Sigmoid function\n",
        "        x = self.embed(input) # Dim: L x B x E\n",
        "        x = self.dropout(x)\n",
        "        hidden = torch.sigmoid(x.sum(axis=0)) # Dim: B x E\n",
        "        \n",
        "        # ---------------------------------\n",
        "        # === Classification layer ===\n",
        "        logit = self.fc(hidden) # Dim: Batch_size x Class_size\n",
        "        # Normalization\n",
        "        logprob = F.log_softmax(logit, dim=1)\n",
        "        \n",
        "        # ---------------------------------\n",
        "        # === Loss function ===\n",
        "        # Compute negative log-likelihood loss\n",
        "        loss = F.cross_entropy(logprob, label)\n",
        "        return loss, logprob\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "093e9cOOxYiv"
      },
      "source": [
        "### 2.2 A Convolutional Neural Network Classifier\n",
        "\n",
        "In this section, we will build a CNN classifier based on the specific instructions given in the homework assignment. Mostly, we re-use the data processing code provided in section 1 and the training code provided in section 2.3. In order to train the CNN classifier instead of the simple feedforward neural network classifiers, \n",
        "you need to replace code line 7 in section 2.2.2 with the following line:\n",
        "\n",
        "```Python\n",
        "    model = NeuralClassifier(vocab_size, embed_size=64, drop_rate=0.0, class_size=2)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL_m5EaAxYiw"
      },
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, drop_rate=0.0,\n",
        "                 class_size=2, kernel_sizes=[2,3,4],\n",
        "                 dropout=0.0, using_cuda=False, pad=None):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # ---------------------------------\n",
        "        # Configuration\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.class_size = class_size # number of classes\n",
        "        self.kernel_sizes = kernel_sizes # a list of kernel sizes\n",
        "        self.using_cuda = using_cuda\n",
        "        self.pad = pad # index of <pad> in vocab\n",
        "        # ---------------------------------\n",
        "        # Model Arch\n",
        "        # Remove the following line and define some parameters for the CNN model here\n",
        "        raise NotImplementedError(\"Not done yet\")\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # ===============================\n",
        "        # Implement the CNN specified in the homework\n",
        "        \n",
        "        # ===============================\n",
        "        \n",
        "        logprob = F.log_softmax(logit, dim=1)\n",
        "        loss = F.cross_entropy(logprob, label)\n",
        "        return loss, logprob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6Pc1cHxYiw"
      },
      "source": [
        "## 2.3 Neural Network Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw_iFk_jxYix"
      },
      "source": [
        "### 2.2.1 Additional functions for mini-batch training and evaluation\n",
        "\n",
        "Define the training function with a mini-batch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGXCU5t5xYix"
      },
      "source": [
        "\n",
        "def batch_train(batch, model, optimizer):\n",
        "    \"\"\" Training with one batch\n",
        "    - batch: a min-batch of the data\n",
        "    - model: the defined neural network\n",
        "    - optimizer: optimization method used to update the parameters\n",
        "    \"\"\"\n",
        "    # set in training mode\n",
        "    model.train()\n",
        "    # initialize optimizer\n",
        "    optimizer.zero_grad()\n",
        "    # forward: prediction\n",
        "    loss, _ = model(batch)\n",
        "    # backward: gradient computation\n",
        "    loss.backward()\n",
        "    # norm clipping, in case the gradient norm is too large\n",
        "    clip_grad_norm(model.parameters(), grad_clip)\n",
        "    # gradient-based update parameter\n",
        "    optimizer.step()\n",
        "    return model, loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekPdScG_xYix"
      },
      "source": [
        "Define the evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T67PggXxYiy"
      },
      "source": [
        "def eval(data_iter, model):\n",
        "    \"\"\" Evaluate the model with the data\n",
        "    data_iter: the data iterator \n",
        "    model: the defined model\n",
        "    \"\"\"\n",
        "    # set in the eval model, which will trun off the features only used for training, such as droput\n",
        "    model.eval()\n",
        "    # records\n",
        "    val_loss, val_batch = 0, 0\n",
        "    total_example, correct_pred = 0, 0\n",
        "    # iterate all the mini batches for evaluation\n",
        "    for b, batch in enumerate(data_iter):\n",
        "        # Forward: prediction\n",
        "        loss, logprob = model(batch)\n",
        "        # \n",
        "        val_batch += 1\n",
        "        val_loss += loss\n",
        "        # Argmax\n",
        "        max_logprob, pred_label = torch.max(logprob, -1)\n",
        "        correct_pred += (pred_label==batch.label).sum()\n",
        "        total_example += batch.label.size()[0]\n",
        "    acc = (1.0*correct_pred)/total_example\n",
        "    # print(\"val_batch = {}\".format(val_batch))\n",
        "    return (val_loss/val_batch), acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfTdi_YUxYiy"
      },
      "source": [
        "### 2.2.2 Main function for training\n",
        "\n",
        "There are four blocks in the following code\n",
        "\n",
        "1. Reset the random seed to make sure that everytime you run the following code will get the same results (with the same hyper-parameters)\n",
        "2. Define the model and optimizers. Potentially useful hyper-parameters in this block\n",
        "    - **embed_size**: Word embedding size, default value: 64\n",
        "    - **drop_rate**: Dropout rate, default value: 0 (no dropout)\n",
        "    - **lr**: Initial learning rate ($\\eta_0$), default value: 0.1\n",
        "    - **weight_decay**: $\\ell_2$ regularization parameter, default value: 0 (no $\\ell_2$ regularization)\n",
        "3. Define the numbers of total training epochs and the number of updating steps for calling the validation function. Potentially useful hyper-parameters in this block\n",
        "    - **epoch**: Total training epoch, default value: 5\n",
        "    - **val_step**: Validation steps, default value: 50\n",
        "4. The main function of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LhD3UJkNxYiz"
      },
      "source": [
        "# -----------------------------------\n",
        "# 1. Random seed\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# ------------------------------------\n",
        "# 2. Define the model and optimizer\n",
        "# 'ffn': Feed-forward network; 'cnn': Convolutional neural network\n",
        "model_name = 'ffn' \n",
        "if model_name == 'ffn':\n",
        "    model = NeuralClassifier(vocab_size, embed_size=64, drop_rate=0.0, class_size=2)\n",
        "elif model_name == 'cnn':\n",
        "    model = SimpleCNN(vocab_size, embed_size=64, drop_rate=0.0, class_size=2)\n",
        "else:\n",
        "    raise ValueError(\"Unrecognized model name\")\n",
        "# optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-1, weight_decay=0)\n",
        "# the norm of grad clipping\n",
        "grad_clip = 1.0\n",
        "\n",
        "# ------------------------------------\n",
        "# 3. Define the numbers of training epochs and validation steps\n",
        "epoch, val_step = 5, 50\n",
        "\n",
        "# ------------------------------------\n",
        "# 4. Training iterations\n",
        "TrnLoss, ValLoss, ValAcc = [], [], []\n",
        "total_batch = 0\n",
        "for e in trange(epoch):\n",
        "    # print(e)\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        total_batch += 1\n",
        "        # Update parameters with one batch\n",
        "        model, loss = batch_train(batch, model, optimizer)\n",
        "        # Compute validation loss after each val_step\n",
        "        if total_batch % val_step == 0:\n",
        "            val_loss, val_acc = eval(val_iter, model)\n",
        "            ValLoss.append(val_loss)\n",
        "            ValAcc.append(val_acc)\n",
        "            TrnLoss.append(loss)\n",
        "print(\"The best validation accuracy = {:.4}\".format(max(ValAcc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m33DZO_nxYiz"
      },
      "source": [
        "Plot the training and validation losses along the training steps (each step involves updating with one mini-batch of the training examples). \n",
        "\n",
        "Although the validation loss *fluctuates* along with updating, the over tendency is decreasing, which means the model is learning some information that is useful for prediction.\n",
        "\n",
        "An alterative way of visualizing the following curves dynamically is using [tensorboard](https://pytorch.org/docs/stable/tensorboard.html), which will not be covered in this class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-T5velmxYiz"
      },
      "source": [
        "plt.plot(range(len(TrnLoss)), TrnLoss, color=\"red\", label=\"Training Loss\") # Training loss\n",
        "plt.plot(range(len(ValLoss)), ValLoss, color=\"blue\", label=\"Develoopment Loss\") # Val loss\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"NLL\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}